{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":821251,"sourceType":"datasetVersion","datasetId":430934}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-28T19:43:50.476709Z","iopub.execute_input":"2024-02-28T19:43:50.477071Z","iopub.status.idle":"2024-02-28T19:43:51.424681Z","shell.execute_reply.started":"2024-02-28T19:43:50.477043Z","shell.execute_reply":"2024-02-28T19:43:51.423755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Goals for this session\n### 1. Import data\n### 2. Explore data\n### 3. Clean Data\n### 4. Extract important sub datasets from the main dataset\n### 5. Decide on what models to use to get what output","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:43:51.426250Z","iopub.execute_input":"2024-02-28T19:43:51.426620Z","iopub.status.idle":"2024-02-28T19:43:51.430385Z","shell.execute_reply.started":"2024-02-28T19:43:51.426594Z","shell.execute_reply":"2024-02-28T19:43:51.429695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cloud & Big Data\n\n##\n\n\n## Experiential Learning\n\n","metadata":{}},{"cell_type":"markdown","source":"### 1. Problem Statement\nPredictive Analytics for Retail\nDevelop a model that predicts consumer behavior in the retail sector based on historical data. This project involves using machine learning algorithms to forecast trends and optimize inventory management.\n\n","metadata":{}},{"cell_type":"markdown","source":"### 2. Data Collection\nThis data was collected from the UCI ML Repository.\n\n\nThe following additional data is given along with the dataset:\n\n**Additional Dataset Information:**\nThis Online Retail II data set contains all the transactions occurring for a UK-based and registered, non-store online retail between 01/12/2009 and 09/12/2011.The company mainly sells unique all-occasion gift-ware. Many customers of the company are wholesalers.\n\n**Additional Variable Information**\nInvoiceNo: Invoice number. Nominal. A 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'c', it indicates a cancellation. \nStockCode: Product (item) code. Nominal. A 5-digit integral number uniquely assigned to each distinct product. \nDescription: Product (item) name. Nominal. \nQuantity: The quantities of each product (item) per transaction. Numeric.\t\nInvoiceDate: Invice date and time. Numeric. The day and time when a transaction was generated. \nUnitPrice: Unit price. Numeric. Product price per unit in sterling (Â£). \nCustomerID: Customer number. Nominal. A 5-digit integral number uniquely assigned to each customer. \nCountry: Country name. Nominal. The name of the country where a customer resides.","metadata":{}},{"cell_type":"code","source":"# Import all the necessary libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:43:51.431512Z","iopub.execute_input":"2024-02-28T19:43:51.431839Z","iopub.status.idle":"2024-02-28T19:43:52.963409Z","shell.execute_reply.started":"2024-02-28T19:43:51.431808Z","shell.execute_reply":"2024-02-28T19:43:52.962434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let's import the data\n\ndf = pd.read_csv(\"/kaggle/input/online-retail-ii-uci/online_retail_II.csv\", header = 0) \n\n# Good practice to make a copy of the original dataset so we don't tamper with the original one\nretail_data = df.copy()\n\n# Let's see what it looks like\nretail_data","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:43:52.965564Z","iopub.execute_input":"2024-02-28T19:43:52.965987Z","iopub.status.idle":"2024-02-28T19:43:55.477718Z","shell.execute_reply.started":"2024-02-28T19:43:52.965961Z","shell.execute_reply":"2024-02-28T19:43:55.476716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We see that there are 1067371 rows and 8 columns\n# Those 8 columns are:\n# Invoice: Invoice number. Nominal. A 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'c', it indicates a cancellation.\n# StockCode: Product (item) code. Nominal. A 5-digit integral number uniquely assigned to each distinct product.\n# Description: Product (item) name. Nominal.\n# Quantity: The quantities of each product (item) per transaction. Numeric.\n# InvoiceDate: Invice date and time. Numeric. The day and time when a transaction was generated.\n# UnitPrice: Unit price. Numeric. Product price per unit in sterling (Â£).\n# CustomerID: Customer number. Nominal. A 5-digit integral number uniquely assigned to each customer.\n# Country: Country name. Nominal. The name of the country where a customer resides.","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:43:55.478907Z","iopub.execute_input":"2024-02-28T19:43:55.479218Z","iopub.status.idle":"2024-02-28T19:43:55.483619Z","shell.execute_reply.started":"2024-02-28T19:43:55.479194Z","shell.execute_reply":"2024-02-28T19:43:55.482736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let us look are more info about the dataset\n\ndef get_summary(dframe):\n    summary_info = pd.DataFrame({\n        'ColumnName': dframe.columns,\n        'DataType': dframe.dtypes,\n        'Values': len(dframe),\n        'UniqueValues': dframe.nunique(),\n        'NullValues': dframe.isnull().sum(),\n        'NullPercentage': (dframe.isnull().sum() / len(dframe)) * 100\n        })\n    # Adjusting 'Rows' to be the same for all columns for consistency\n    summary_info['Values'] = len(retail_data)\n    return summary_info\n\nget_summary(retail_data)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:43:55.484826Z","iopub.execute_input":"2024-02-28T19:43:55.485199Z","iopub.status.idle":"2024-02-28T19:43:56.924423Z","shell.execute_reply.started":"2024-02-28T19:43:55.485163Z","shell.execute_reply":"2024-02-28T19:43:56.923522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here we see 53628 unique orders (Invoice), 5305 unique products, 5924 unique customers and 43 unique countries.\n\n# We also see that there are 4382 null values in description. Which we'll have to deal with.\n# We also have 243007 null values in customer ID, which we really need to handle before we do any kind of feature engineering or modelling.\n\n# Also, the Invoice date is in object format. Let's convert that to datetime\nretail_data['InvoiceDate']= pd.to_datetime(retail_data['InvoiceDate'])\n\nget_summary(retail_data)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:43:56.925493Z","iopub.execute_input":"2024-02-28T19:43:56.925778Z","iopub.status.idle":"2024-02-28T19:43:58.274395Z","shell.execute_reply.started":"2024-02-28T19:43:56.925754Z","shell.execute_reply":"2024-02-28T19:43:58.273420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's look at more info\n# How many customers, products, transactions, countries\n# How many returned and cancelled orders\n# Can we check if there are returned orders among the cancelled orders?\n# Stock Code to see what kinds of transactions (Discount, Postage, Manual, etc)\n\norder_stats = pd.DataFrame([{\n    'Customers': len(retail_data['Customer ID'].value_counts()),\n    'Products': len(retail_data['StockCode'].value_counts()),\n    'Transactions': len(retail_data['Invoice'].value_counts()),\n    'Countries': len(retail_data['Country'].value_counts()),\n    'CancelledOrders': len(retail_data[retail_data['Invoice'].str.contains('C')].value_counts()),\n    }], columns = ['Customers', 'Products', 'Transactions', 'Countries', 'CancelledOrders'], index = ['Quantity'])\norder_stats","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:43:58.275776Z","iopub.execute_input":"2024-02-28T19:43:58.276266Z","iopub.status.idle":"2024-02-28T19:43:59.263143Z","shell.execute_reply.started":"2024-02-28T19:43:58.276232Z","shell.execute_reply":"2024-02-28T19:43:59.262134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let's do some feature engineering. What are the additional information do we need?\n# 1. Total Price\n\nretail_fe = retail_data.copy()\nretail_fe['TotalPrice'] = retail_fe['Price'] * retail_fe['Quantity']\n\nretail_fe","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:43:59.264229Z","iopub.execute_input":"2024-02-28T19:43:59.264511Z","iopub.status.idle":"2024-02-28T19:43:59.472248Z","shell.execute_reply.started":"2024-02-28T19:43:59.264488Z","shell.execute_reply":"2024-02-28T19:43:59.471317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A few things to clean up.\n## We have many cancelled orders, which we could drop\n## We also have to deal with the special codes, that correspond to other transactions like POSTAGE, etc.\n\n## 1. Let's look at special StockCodes\nlist_special_codes = retail_fe[retail_fe['StockCode'].str.contains('^[a-zA-Z]+', regex=True)]['StockCode'].unique()\n## 2. Let's look at cancelled orders, looking at Invoice\nlist_cancel_codes = retail_fe[retail_fe['Invoice'].str.startswith('C')]['Invoice'].unique()\n\nprint(len(list_special_codes))\nprint(len(list_cancel_codes))","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:43:59.475757Z","iopub.execute_input":"2024-02-28T19:43:59.476041Z","iopub.status.idle":"2024-02-28T19:44:00.610800Z","shell.execute_reply.started":"2024-02-28T19:43:59.476017Z","shell.execute_reply":"2024-02-28T19:44:00.609895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retail_cleaned = retail_fe[~retail_fe['StockCode'].isin(list_special_codes)].reset_index(drop = True)\nretail_cleaned = retail_cleaned[~retail_cleaned['Invoice'].isin(list_cancel_codes)].sort_values(by = 'Invoice', ascending = True).reset_index(drop = True)\nretail_cleaned","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:44:00.612001Z","iopub.execute_input":"2024-02-28T19:44:00.612277Z","iopub.status.idle":"2024-02-28T19:44:02.211043Z","shell.execute_reply.started":"2024-02-28T19:44:00.612253Z","shell.execute_reply":"2024-02-28T19:44:02.210128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let's look at some aggregate values (works like Pivot Table)\n\n# Let's create a DataFrame to store the data by Invoice ID\n# Aggregate by 'Invoice' to calculate total basket price\ninvoice_summary = retail_cleaned.groupby('Invoice').agg(\n    BasketPrice = ('TotalPrice', 'sum'),\n    InvoiceDate=('InvoiceDate', 'first') # Assuming each invoice has a single date\n).reset_index()\n\ninvoice_summary.sort_values(by = 'BasketPrice', ascending = False)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:44:02.212226Z","iopub.execute_input":"2024-02-28T19:44:02.212502Z","iopub.status.idle":"2024-02-28T19:44:02.383965Z","shell.execute_reply.started":"2024-02-28T19:44:02.212478Z","shell.execute_reply":"2024-02-28T19:44:02.383059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's set the InvoiceDate as the index to make things easier for a time series analysis\ninvoice_summary.set_index('InvoiceDate', inplace = True)\n# Let's add some more features, like time values like Day, month, year\ninvoice_summary['Year'] = invoice_summary.index.year\ninvoice_summary['Month'] = invoice_summary.index.month\ninvoice_summary['Week'] = invoice_summary.index.isocalendar().week\ninvoice_summary['DayOfYear'] = invoice_summary.index.dayofyear\ninvoice_summary['Date'] = invoice_summary.index.date\n\ninvoice_summary","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:44:02.385094Z","iopub.execute_input":"2024-02-28T19:44:02.385369Z","iopub.status.idle":"2024-02-28T19:44:02.426377Z","shell.execute_reply.started":"2024-02-28T19:44:02.385344Z","shell.execute_reply":"2024-02-28T19:44:02.425548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ts_date = invoice_summary.groupby('Date').agg(\n    DayOfYear = ('DayOfYear', 'first'), \n    Year = ('Year', 'first'),\n    Month = ('Month', 'first'),\n    Week = ('Week', 'first'),\n    BasketPrice = ('BasketPrice', 'sum')\n)\nts_date","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:44:02.427318Z","iopub.execute_input":"2024-02-28T19:44:02.427564Z","iopub.status.idle":"2024-02-28T19:44:02.463039Z","shell.execute_reply.started":"2024-02-28T19:44:02.427542Z","shell.execute_reply":"2024-02-28T19:44:02.462247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 8)) \nplt.plot(ts_date.index, ts_date['BasketPrice'], label='Basket Price')\n\nplt.title('Basket Price Whole Picture') \nplt.xlabel('Date')\nplt.ylabel('Basket Price') \nplt.xticks(rotation=45)\nplt.legend()\nplt.show()  # Displays the plot","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:44:02.464163Z","iopub.execute_input":"2024-02-28T19:44:02.464616Z","iopub.status.idle":"2024-02-28T19:44:02.924448Z","shell.execute_reply.started":"2024-02-28T19:44:02.464584Z","shell.execute_reply":"2024-02-28T19:44:02.923469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.color_palette()","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:44:02.926140Z","iopub.execute_input":"2024-02-28T19:44:02.926608Z","iopub.status.idle":"2024-02-28T19:44:02.934292Z","shell.execute_reply.started":"2024-02-28T19:44:02.926572Z","shell.execute_reply":"2024-02-28T19:44:02.933421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 8)) \n\nfor year in [2009, 2010, 2011]:\n    filtered_df = ts_date[ts_date['Year'] == year]\n    plt.plot(filtered_df['DayOfYear'], filtered_df['BasketPrice'], label=str(year))\n\n\nplt.title('Basket Price Each Year') \nplt.xlabel('Date')\nplt.ylabel('Basket Price') \nplt.xticks(rotation=45)\nplt.legend()\nplt.tight_layout()  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:44:02.935530Z","iopub.execute_input":"2024-02-28T19:44:02.935853Z","iopub.status.idle":"2024-02-28T19:44:03.317395Z","shell.execute_reply.started":"2024-02-28T19:44:02.935828Z","shell.execute_reply":"2024-02-28T19:44:03.316480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For simplicity's sake let's remove the 2009 data\nts = ts_date[ts_date['Year'] != 2009]\n\nplt.figure(figsize=(15, 8)) \n\n\nfor year in [2010, 2011]:\n    filtered_df = ts[ts['Year'] == year]\n    plt.plot(filtered_df['DayOfYear'], filtered_df['BasketPrice'], label=str(year))\n\nplt.title('Basket Price Each Year') \nplt.xlabel('Date')\nplt.ylabel('Basket Price') \nplt.xticks(rotation=45)\nplt.legend()\nplt.tight_layout()  \nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:44:03.318762Z","iopub.execute_input":"2024-02-28T19:44:03.319050Z","iopub.status.idle":"2024-02-28T19:44:03.684738Z","shell.execute_reply.started":"2024-02-28T19:44:03.319024Z","shell.execute_reply":"2024-02-28T19:44:03.683859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_df = ts[((ts['Month'] > 1 ) & (ts['Month'] < 3 )) & (ts['Year'] == 2010)] \nplt.plot(filtered_df.index, filtered_df['BasketPrice'])\nplt.xticks(rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:44:03.685933Z","iopub.execute_input":"2024-02-28T19:44:03.686282Z","iopub.status.idle":"2024-02-28T19:44:03.905574Z","shell.execute_reply.started":"2024-02-28T19:44:03.686249Z","shell.execute_reply":"2024-02-28T19:44:03.904765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the model\n## This is called XG Boost (eXtreme Gradient Boost) and is beneficial for complex data\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Let's do a train/test split\nxgb_X_train = ts[ts['Year'] == 2010][['DayOfYear', 'Week', 'Month', 'Year']]\nxgb_y_train = ts[ts['Year'] == 2010]['BasketPrice']\nxgb_X_test = ts[ts['Year'] == 2011][['DayOfYear', 'Week', 'Month', 'Year']]\nxgb_y_test = ts[ts['Year'] == 2011]['BasketPrice']\n\n# Let's train the XGBoost model\nmodel_xgb = XGBRegressor(objective='reg:squarederror')\nmodel_xgb.fit(xgb_X_train, xgb_y_train, eval_set=[(xgb_X_test, xgb_y_test)], verbose=5)\n\n# Predict\npredictions_xgb = model_xgb.predict(xgb_X_test)\n\nrmse_xgb = mean_squared_error(xgb_y_test, predictions_xgb, squared = False)\nprint(f'XGBoost RMSE: {rmse_xgb}')","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:44:03.906800Z","iopub.execute_input":"2024-02-28T19:44:03.907161Z","iopub.status.idle":"2024-02-28T19:44:04.082348Z","shell.execute_reply.started":"2024-02-28T19:44:03.907127Z","shell.execute_reply":"2024-02-28T19:44:04.081556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting for XGBoost\nplt.figure(figsize=(10, 6))\nplt.plot(xgb_X_test.index, xgb_y_test, label='Actual 2011')\nplt.plot(xgb_X_test.index, predictions_xgb, label='XGBoost Predictions 2011', alpha=0.7)\nplt.legend()\nplt.title('XGBoost Predictions vs Actual 2011')\nplt.show()\n\n# RMSE for XGBoost\nprint(f'XGBoost RMSE: {rmse_xgb}')","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:44:04.083358Z","iopub.execute_input":"2024-02-28T19:44:04.083834Z","iopub.status.idle":"2024-02-28T19:44:04.377032Z","shell.execute_reply.started":"2024-02-28T19:44:04.083806Z","shell.execute_reply":"2024-02-28T19:44:04.376133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get feature importances for XGBoost\nfi = pd.DataFrame(data = model_xgb.feature_importances_,\n                                 index = model_xgb.feature_names_in_,\n                                 columns = ['Importance']\n                                )\nfi.sort_values(by = 'Importance', ascending = True).plot(kind = 'barh', title = 'Feature Importances', grid = True)\nprint(fi)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:44:04.378202Z","iopub.execute_input":"2024-02-28T19:44:04.378487Z","iopub.status.idle":"2024-02-28T19:44:04.575197Z","shell.execute_reply.started":"2024-02-28T19:44:04.378463Z","shell.execute_reply":"2024-02-28T19:44:04.574296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's run another model training. This time let's add both the years, and let's have it predict the next year.\n\n# Combine 2010 and 2011 for training\nxgb_2_X_train = ts[ts['Year'].isin([2010, 2011])][['DayOfYear', 'Week', 'Month', 'Year']]\nxgb_2_y_train = ts[ts['Year'].isin([2010, 2011])]['BasketPrice']\n\n# Create a dummy 2012 feature set\n# Assuming 2012 is similar to previous years, and repeating the pattern\ndates_2012 = pd.date_range(start=\"2012-01-01\", end=\"2012-12-31\")\nxgb_2_X_predict = pd.DataFrame({\n    'DayOfYear': dates_2012.dayofyear,\n    'Week': dates_2012.isocalendar().week,\n    'Month': dates_2012.month,\n    'Year': 2012\n})\n\n# Train the XGBoost model\nmodel_2_xgb = XGBRegressor(objective='reg:squarederror')\nmodel_2_xgb.fit(xgb_2_X_train, xgb_2_y_train)\n\n# Predict for 2012\npredictions_2012 = model_2_xgb.predict(xgb_2_X_predict)\npredictions_2012","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:44:23.132641Z","iopub.execute_input":"2024-02-28T19:44:23.133414Z","iopub.status.idle":"2024-02-28T19:44:23.210977Z","shell.execute_reply.started":"2024-02-28T19:44:23.133378Z","shell.execute_reply":"2024-02-28T19:44:23.210197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let us create a dataframe that is similar to existing values, and append the predicted values.\npredicted_2012_df = pd.DataFrame(index=dates_2012, columns=ts_date.columns)\n\n# Since we only have predictions for the 'BasketPrice', we fill this column with our predictions\npredicted_2012_df['BasketPrice'] = predictions_2012\n\n# All other product columns should remain NaN as we don't have predictions for individual products\n# Assuming other necessary columns like 'DayOfYear', 'Week', 'Month', and 'Year' should be filled according to 2012\npredicted_2012_df['DayOfYear'] = predicted_2012_df.index.dayofyear\npredicted_2012_df['Week'] = predicted_2012_df.index.isocalendar().week\npredicted_2012_df['Month'] = predicted_2012_df.index.month\npredicted_2012_df['Year'] = 2012\npredicted_2012_df['Date'] = predicted_2012_df.index.date\npredicted_2012_df.set_index('Date', inplace = True)\n\n# For columns like 'Invoice', 'CustomerID', 'Country' which are NaN because we don't have actual data\n# They should stay as NaN in predicted_2012_df\n\n# Append the predicted_2012_df to the original ts_date dataframe\nts_date_extended = pd.concat([ts_date, predicted_2012_df])\n\n# Rename 'BasketPrice' in predicted_2012_df to 'Prediction_2012' and keep it in the new column\nts_date_extended['Prediction_2012'] = ts_date_extended['BasketPrice'].where(ts_date_extended['Year'] == 2012)\nts_date_extended","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:44:29.448624Z","iopub.execute_input":"2024-02-28T19:44:29.449434Z","iopub.status.idle":"2024-02-28T19:44:29.473668Z","shell.execute_reply.started":"2024-02-28T19:44:29.449404Z","shell.execute_reply":"2024-02-28T19:44:29.472651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As we can see, because of lack of values, the model has given negative values at 2012-12-31. We can clip that\nts_date_extended['Prediction_2012'] = ts_date_extended['Prediction_2012'].clip(lower=0)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:48:59.849775Z","iopub.execute_input":"2024-02-28T19:48:59.850405Z","iopub.status.idle":"2024-02-28T19:48:59.856313Z","shell.execute_reply.started":"2024-02-28T19:48:59.850374Z","shell.execute_reply":"2024-02-28T19:48:59.855387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 8)) \n\nfor year in [2010, 2011]:\n    filtered_df = ts_date_extended[ts_date_extended['Year'] == year]\n    plt.plot(filtered_df['DayOfYear'], filtered_df['BasketPrice'], label=str(year))\n\nts_2012 = ts_date_extended[ts_date_extended['Year'] == 2012]    \nplt.plot(ts_2012['DayOfYear'], ts_2012['Prediction_2012'], label = 'Prediction for 2012')\n\nplt.title('Basket Price Each Year with prediction') \nplt.xlabel('Date')\nplt.ylabel('Basket Price') \nplt.xticks(rotation=45)\nplt.legend()\nplt.tight_layout()  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:49:02.786152Z","iopub.execute_input":"2024-02-28T19:49:02.786999Z","iopub.status.idle":"2024-02-28T19:49:03.157047Z","shell.execute_reply.started":"2024-02-28T19:49:02.786965Z","shell.execute_reply":"2024-02-28T19:49:03.156117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As we can see that our model has predicted pretty accurate values for 2012, based on 2010 and 2012 data.\n## But we don't can't validate its accuracy because we don't have actual values.","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:44:05.201737Z","iopub.execute_input":"2024-02-28T19:44:05.202034Z","iopub.status.idle":"2024-02-28T19:44:05.205524Z","shell.execute_reply.started":"2024-02-28T19:44:05.201992Z","shell.execute_reply":"2024-02-28T19:44:05.204506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's look at the feature importances\n# Get feature importances for XGBoost\nfi_2 = pd.DataFrame(data = model_2_xgb.feature_importances_,\n                                 index = model_xgb.feature_names_in_,\n                                 columns = ['Importance']\n                                )\nfi_2.sort_values(by = 'Importance', ascending = True).plot(kind = 'barh', title = 'Feature Importances', grid = True)\nprint(fi)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-28T19:49:30.447301Z","iopub.execute_input":"2024-02-28T19:49:30.447655Z","iopub.status.idle":"2024-02-28T19:49:30.626190Z","shell.execute_reply.started":"2024-02-28T19:49:30.447627Z","shell.execute_reply":"2024-02-28T19:49:30.625270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It gives similar importances as the previous model. ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let us do something more\n## Let us try to predict which products are bought from which customers from which countries.\n\n# For that let's only consider the top 30 products, top 30 customers and top 10 countries and see who buys how much and when.\n\n#So we'll have to prune the data to make it more manageable\n\n# First, we will identify the top 30 selling products based on the total quantity sold.\n\n# Group by StockCode and sum quantities, then get the top 50.\ntop_products = retail_cleaned.groupby('StockCode').agg({'Quantity': 'sum'}).nlargest(50, 'Quantity')\n\n# Now, identify the top 10 countries by sales volume.\ntop_countries = retail_cleaned.groupby('Country').agg({'TotalPrice': 'sum'}).nlargest(10, 'TotalPrice')\n\n# Lastly, identify the top 30 customers by sales volume.\ntop_customers = retail_cleaned.groupby('Customer ID').agg({'TotalPrice': 'sum'}).nlargest(30, 'TotalPrice')\n\n# Filter the original cleaned dataset to include only the entries corresponding to the top categories identified.\nfiltered_data = retail_cleaned[\n    (retail_cleaned['StockCode'].isin(top_products.index)) &\n    (retail_cleaned['Country'].isin(top_countries.index)) &\n    (retail_cleaned['Customer ID'].isin(top_customers.index))\n].reset_index(drop = True)\nfiltered_data","metadata":{"execution":{"iopub.status.busy":"2024-02-28T20:27:24.125087Z","iopub.execute_input":"2024-02-28T20:27:24.125445Z","iopub.status.idle":"2024-02-28T20:27:24.489124Z","shell.execute_reply.started":"2024-02-28T20:27:24.125418Z","shell.execute_reply":"2024-02-28T20:27:24.488261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_summary(filtered_data)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T20:27:27.907913Z","iopub.execute_input":"2024-02-28T20:27:27.908526Z","iopub.status.idle":"2024-02-28T20:27:27.931446Z","shell.execute_reply.started":"2024-02-28T20:27:27.908495Z","shell.execute_reply":"2024-02-28T20:27:27.930638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will pivot the filtered data to get the required structure:\n# - One column for each of the top 30 products indicating the quantity purchased in that invoice.\n# - One binary column for each of the top 10 countries indicating if the order was placed from that country.\n# - One binary column for each of the top 30 customers indicating if the order was made by that customer.\n# - Columns for 'BasketPrice' and 'InvoiceDate'.\n\n# Preparing product quantity columns\nproduct_pivot = filtered_data.pivot_table(index='Invoice', columns='StockCode', values='Quantity', aggfunc='sum', fill_value=0)\n\n# Preparing binary columns for countries\ncountry_pivot = pd.get_dummies(filtered_data['Country']).groupby(filtered_data['Invoice']).max()\n\n# Preparing binary columns for customers\ncustomer_pivot = pd.get_dummies(filtered_data['Customer ID'], prefix='Customer').groupby(filtered_data['Invoice']).max()\n\n# Calculating BasketPrice for each invoice\nbasket_price = filtered_data.groupby('Invoice').agg({'TotalPrice': 'sum'}).rename(columns={'TotalPrice': 'BasketPrice'})\n\n# Adding InvoiceDate\ninvoice_date = filtered_data.groupby('Invoice').agg({'InvoiceDate': 'first'})\n\n# Concatenating all the pivots and the basket price to form the final aggregated dataframe\naggregated_data = pd.concat([product_pivot, country_pivot, customer_pivot, basket_price, invoice_date], axis=1)\n\n# Checking the structure of the resulting dataframe\naggregated_data.head(), aggregated_data.shape","metadata":{"execution":{"iopub.status.busy":"2024-02-28T20:36:07.876832Z","iopub.execute_input":"2024-02-28T20:36:07.877531Z","iopub.status.idle":"2024-02-28T20:36:07.927726Z","shell.execute_reply.started":"2024-02-28T20:36:07.877496Z","shell.execute_reply":"2024-02-28T20:36:07.926903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}